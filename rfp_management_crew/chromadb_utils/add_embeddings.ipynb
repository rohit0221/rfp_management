{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed proposal_1.pdf_chunk_0 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_1 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_2 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_3 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_4 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_5 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_6 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_7 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_8 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_9 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_10 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_11 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_12 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_13 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_14 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_15 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_1.pdf_chunk_16 | Supplier: CloudTech Solutions\n",
      "✅ Processed proposal_2.pdf_chunk_0 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_1 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_2 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_3 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_4 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_5 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_6 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_7 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_8 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_9 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_10 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_2.pdf_chunk_11 | Supplier: CloudNexus Technologies \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_0 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_1 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_2 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_3 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_4 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_5 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_6 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_7 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_8 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_9 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_10 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_11 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Processed proposal_3.pdf_chunk_12 | Supplier: SkyCloud Innovations \n",
      "Headquarters\n",
      "✅ Successfully stored 42 chunks in ChromaDB with embeddings and metadata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import chromadb\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings  # Use the updated package for embeddings\n",
    "\n",
    "# Load environment variables (ensure OPENAI_API_KEY and EMBEDDING_MODEL are set in your .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client (for generating embeddings)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def extract_metadata(text):\n",
    "    \"\"\"\n",
    "    Extracts supplier metadata from text using regex.\n",
    "    Returns a dictionary with supplier, contact_person, and email.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    supplier_match = re.search(r\"Company Name:\\s*([\\w\\s]+)\", text)\n",
    "    contact_match = re.search(r\"Contact:\\s*([\\w\\s]+)\", text)\n",
    "    email_match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text)\n",
    "    \n",
    "    metadata[\"supplier\"] = supplier_match.group(1).strip() if supplier_match else \"Unknown\"\n",
    "    metadata[\"contact_person\"] = contact_match.group(1).strip() if contact_match else \"Unknown\"\n",
    "    metadata[\"email\"] = email_match.group(0) if email_match else \"Unknown\"\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def process_and_store_pdfs():\n",
    "    \"\"\"\n",
    "    Reads PDF files from a directory, extracts full text and metadata,\n",
    "    splits text into chunks, generates embeddings manually using OpenAIEmbeddings,\n",
    "    and stores documents, embeddings, and metadata in ChromaDB.\n",
    "    \"\"\"\n",
    "    pdf_dir = \"../data/proposals/\"  # Adjust this path as needed\n",
    "    chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "    \n",
    "    # Create or get a collection without a built-in embedding function (since we're generating embeddings manually)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"rfp_proposals\")\n",
    "    \n",
    "    # Set up the text splitter for chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    # Lists for batch insertion into ChromaDB\n",
    "    all_ids, all_documents, all_embeddings, all_metadatas = [], [], [], []\n",
    "    \n",
    "    # Process each PDF in the directory\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_dir, filename)\n",
    "            doc = fitz.open(file_path)\n",
    "            text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "            \n",
    "            # Extract metadata from the full document text\n",
    "            metadata = extract_metadata(text)\n",
    "            \n",
    "            # Split the full text into chunks\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{filename}_chunk_{i}\"\n",
    "                # Generate embedding using OpenAIEmbeddings (manual embedding)\n",
    "                embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\").embed_query(chunk)\n",
    "                \n",
    "                if embedding is None:\n",
    "                    print(f\"⚠️ Embedding generation failed for {chunk_id}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                all_ids.append(chunk_id)\n",
    "                all_documents.append(chunk)\n",
    "                all_embeddings.append(embedding)\n",
    "                all_metadatas.append(metadata)\n",
    "                \n",
    "                print(f\"✅ Processed {chunk_id} | Supplier: {metadata.get('supplier', 'Unknown')}\")\n",
    "    \n",
    "    # Batch insert into ChromaDB if we have any valid chunks\n",
    "    if all_ids:\n",
    "        collection.add(\n",
    "            ids=all_ids,\n",
    "            documents=all_documents,\n",
    "            embeddings=all_embeddings,\n",
    "            metadatas=all_metadatas\n",
    "        )\n",
    "        print(f\"✅ Successfully stored {len(all_ids)} chunks in ChromaDB with embeddings and metadata.\")\n",
    "    else:\n",
    "        print(\"⚠️ No valid chunks to store.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_store_pdfs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
